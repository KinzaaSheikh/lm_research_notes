{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "IpnzUx0ZF4Og",
        "RVZ5uKmqwjyr",
        "v4Qt7GDnjAQl"
      ],
      "authorship_tag": "ABX9TyNMgcysSq3A2xxHam1oX1Ff",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KinzaaSheikh/lm_research_notes/blob/main/Hands_On_LLM_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Notebook of Hands on LLM book"
      ],
      "metadata": {
        "id": "E8qYI5GaGDXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1"
      ],
      "metadata": {
        "id": "IpnzUx0ZF4Og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduced the recent history of Large Language Models and ended with a small coding example. Its interesting for me to note that a simple query-answer workflow with an LLM is intuitive enough even without using any of the recent frameworks."
      ],
      "metadata": {
        "id": "laX4C0TlM2sv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrF90yIqFztJ"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "hEkNcD6fGAV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline\n",
        "generation = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False\n",
        "    )"
      ],
      "metadata": {
        "id": "8ZSnDJ_vIV_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The prompt (user input / query)\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Tell me a funny joke about chickens\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Generate output\n",
        "output = generation(messages)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "r5WfMN8MIwvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2\n",
        "\n"
      ],
      "metadata": {
        "id": "RVZ5uKmqwjyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization & Embeddings\n",
        "\n",
        "The two main pillars of LLM\n",
        "\n",
        "Tokenization: The smallest chunk a text can be broken down to.\n",
        "\n",
        "\n",
        "Embeddings: The act of converting those tokens into computable language"
      ],
      "metadata": {
        "id": "0b0Jdi-6wl4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq transformers"
      ],
      "metadata": {
        "id": "MXe0KkEMw7K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# load model and tokenizer just like before\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "H4Gf9Xsd9Iq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email to my advisor explaining why I couldn't finish proposal on time. Explain how it happend. <|assistant|>\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Generate the text\n",
        "generation_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=20\n",
        ")\n",
        "\n",
        "# Print the output\n",
        "print(tokenizer.decode(generation_output[0]))"
      ],
      "metadata": {
        "id": "qJDbTTRF9dHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print to see what's inside input_ids\n",
        "\n",
        "print(input_ids)"
      ],
      "metadata": {
        "id": "esaD-uvUAV59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect input ids using tokenizer's decode method\n",
        "# translate the id's back into human readable text\n",
        "\n",
        "for id in input_ids[0]:\n",
        "  print(tokenizer.decode(id))"
      ],
      "metadata": {
        "id": "dStZjjRdAYKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generation_output)"
      ],
      "metadata": {
        "id": "iF7FVsBeA2vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can decode the tokenizer on the output side to translate the token id in actual text\n",
        "print(tokenizer.decode(8496))\n",
        "print(tokenizer.decode(29915))\n",
        "print(tokenizer.decode(29873))"
      ],
      "metadata": {
        "id": "82GC5SB8BNeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode([8496, 29915, 29873]))"
      ],
      "metadata": {
        "id": "P8Db7--iBbd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate contextualized word embeddings\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Load a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "# Load a language model\n",
        "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer('Hello World', return_tensors='pt')\n",
        "\n",
        "# Process the toekns\n",
        "output = model(**tokens)[0]"
      ],
      "metadata": {
        "id": "e4DCyAM1Ci55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "L9QDYov4JnXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect why there are four tokens in two words\n",
        "\n",
        "for token in tokens['input_ids'][0]:\n",
        "  print(tokenizer.decode(token))"
      ],
      "metadata": {
        "id": "7fvbDII3Jt2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "SmQV1CfKKIuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load model\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Convert text to text embeddings\n",
        "vector = model.encode(\"Best movie ever!\")"
      ],
      "metadata": {
        "id": "aKFs6BSIKQB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector.shape"
      ],
      "metadata": {
        "id": "tLQGmuqzLwR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word embeddings beyond LLMs\n",
        "\n",
        "\n",
        "NOTE: Gensim causes a lot of dependency errors so its best to kill the runtime and start over from here"
      ],
      "metadata": {
        "id": "aW8UzChDMAy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gensim"
      ],
      "metadata": {
        "id": "VnSrMOOFKiz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download embeddings (66MB, glove, trained on wikipedia, vector size: 50)\n",
        "# Other options include \"word2vec-google-news-300\"\n",
        "# More options at https://github.com/RaRe-Technologies/gensim-data\n",
        "# Installing specific versions to avoid conflicts\n",
        "import gensim.downloader as api\n",
        "model = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "metadata": {
        "id": "zzbvpvp1MedB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar([model['king']], topn=11)"
      ],
      "metadata": {
        "id": "s9D1URjoNCeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a song embedding model"
      ],
      "metadata": {
        "id": "zDmRBZhBextq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from urllib import request\n",
        "\n",
        "# Get the playlist dataset file\n",
        "data = request.urlopen(\"https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt\")\n",
        "\n",
        "# Parse the playlist dataset file. Skip the first to lines as\n",
        "# they only contian metadata\n",
        "lines = data.read().decode(\"utf-8\").split('\\n')[2:]\n",
        "\n",
        "# Remove playlists with only song\n",
        "playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]\n",
        "\n",
        "# Load song metadata\n",
        "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
        "songs_file = songs_file.read().decode('utf-8').split('\\n')\n",
        "songs = [s.rstrip().split('\\t') for s in songs_file]\n",
        "songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist'])\n",
        "songs_df = songs_df.set_index('id')"
      ],
      "metadata": {
        "id": "dnZylAytdknb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Playlist #1:/n', playlists[0]), '\\n'"
      ],
      "metadata": {
        "id": "Q1l5LFNIgyJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train our Word2Vec model\n",
        "model = Word2Vec(\n",
        "    playlists,\n",
        "    vector_size=32,\n",
        "    window=20,\n",
        "    negative=50,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "ViFtA4RrhGNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "song_id = 2172\n",
        "\n",
        "# Ask the model for songs similar to song number 2172\n",
        "model.wv.most_similar(positive=str(song_id))"
      ],
      "metadata": {
        "id": "AO8O3h-whbav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(songs_df.iloc[2173])"
      ],
      "metadata": {
        "id": "WpYZ1bHqiMZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results are all heavy metal and hard rock, within the same genre\n",
        "import numpy as np\n",
        "\n",
        "def print_recommendations(song_id):\n",
        "  similar_songs = np.array(\n",
        "      model.wv.most_similar(positive=str(song_id), topn=5)\n",
        "  )[:,0]\n",
        "\n",
        "  return songs_df.iloc[similar_songs]\n",
        "\n",
        "# Extract recommendations\n",
        "print_recommendations(2172)"
      ],
      "metadata": {
        "id": "6vRqpYI-icFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 3"
      ],
      "metadata": {
        "id": "v4Qt7GDnjAQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Rerun the first 3 cells of the notebook\n"
      ],
      "metadata": {
        "id": "Kz7CkKGaIBHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "output = generation(prompt)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "OuBV3BpMjCA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display order of layers\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "MlO-4MjhgZT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The capital of France is\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "# Get the output of the model before the lm_head\n",
        "model_output = model.model(input_ids)\n",
        "\n",
        "# Get the output of the lm_head\n",
        "lm_head_output = model.lm_head(model_output[0])"
      ],
      "metadata": {
        "id": "CK5AiqPchsSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_output)"
      ],
      "metadata": {
        "id": "YTLwKOSKC6yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lm_head_output)"
      ],
      "metadata": {
        "id": "hun5hcp_C__9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = lm_head_output[0, -1].argmax(-1)\n",
        "tokenizer.decode(token_id)"
      ],
      "metadata": {
        "id": "r6ARHFuGDG0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output[0].shape"
      ],
      "metadata": {
        "id": "cPNxNc9zDUhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output[0].shape"
      ],
      "metadata": {
        "id": "8P9QytMxDfcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing speed by disabling built-in caching\n",
        "\n",
        "prompt = \"write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids= input_ids.to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "Wrz_bJ1HDhzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 100 tokens with caching and time it\n",
        "\n",
        "%%timeit -n 1\n",
        "generation_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=100,\n",
        "    use_cache=True\n",
        ")"
      ],
      "metadata": {
        "id": "8JiifrDUPymD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the same amount of tokens with caching disabled\n",
        "\n",
        "%%timeit -n 1\n",
        "generation_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=100,\n",
        "    use_cache=False\n",
        ")"
      ],
      "metadata": {
        "id": "Wqpgo2daQCWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4\n"
      ],
      "metadata": {
        "id": "4JUixV5lQx7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets transformers numpy tqdm scikit-learn"
      ],
      "metadata": {
        "id": "rumqP_TqQz3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"rotten_tomatoes\")\n",
        "data"
      ],
      "metadata": {
        "id": "iFKTi_qnRU8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"train\"][0, -1]\n",
        "\n",
        "# Classification of reviews are binary, either 0 (negative) or 1 (positive)"
      ],
      "metadata": {
        "id": "nGiNv7AlRgAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"test\"][0, -1]"
      ],
      "metadata": {
        "id": "VFJkFduuRtkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Path to our HF model\n",
        "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "# Load the mmodel into pipeline\n",
        "pipe = pipeline(\n",
        "    model=model_path,\n",
        "    tokenizer=model_path,\n",
        "    return_all_scores=True,\n",
        "    device=\"cuda:0\"\n",
        ")"
      ],
      "metadata": {
        "id": "CrjWrijYSARw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "\n",
        "# Run inference\n",
        "y_pred = []\n",
        "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")),\n",
        "                  total=len(data[\"test\"])):\n",
        "  negative_score = output[0][\"score\"]\n",
        "  positive_score = output[2][\"score\"]\n",
        "  assignment = np.argmax([negative_score, positive_score])\n",
        "  y_pred.append(assignment)"
      ],
      "metadata": {
        "id": "ZgdIw7AZT-9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After generating predictions its time to evals\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "  \"\"\"Create and print the classification report\"\"\"\n",
        "  performance = classification_report(\n",
        "      y_true,\n",
        "      y_pred,\n",
        "      target_names=[\"Negative Review\", \"Positive Review\"]\n",
        "  )\n",
        "  print(performance)"
      ],
      "metadata": {
        "id": "2XbOrkneVYuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create classification report\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ],
      "metadata": {
        "id": "Wihp47AiYL6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5\n"
      ],
      "metadata": {
        "id": "X6OCJsZRsBVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU umap bertopic"
      ],
      "metadata": {
        "id": "SEOXMyKj1EYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clustering and topic modeling algorithms onn ArXiv articles\n",
        "\n",
        "# Load data from Hugging Face\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"maartengr/arxiv_nlp\")[\"train\"]"
      ],
      "metadata": {
        "id": "gCxXcmrkYV-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract metadata\n",
        "abstracts = dataset[\"Abstracts\"]\n",
        "titles = dataset[\"Titles\"]"
      ],
      "metadata": {
        "id": "An40BPjgta3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Documents"
      ],
      "metadata": {
        "id": "8_uly2mZ1KIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Create an embedding for each abstract\n",
        "embedding_model = SentenceTransformer(\"thenlper/gte-small\")\n",
        "embeddings = embedding_model.encode(abstracts, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "PmpvtcSnuAto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the dimensions of the resulting embeddings\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "4aQF-Vzb0Hdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction"
      ],
      "metadata": {
        "id": "HnDhmi741eSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We reduce the input embeddings from 384 dimensions to 5 dimensions\n",
        "\n",
        "from umap.umap_ import UMAP\n",
        "\n",
        "umap_model = UMAP(\n",
        "    n_components = 5,\n",
        "    min_dist = 0.0,\n",
        "    metric = \"cosine\",\n",
        "    random_state = 42\n",
        ")\n",
        "\n",
        "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
        "\n",
        "# setting random_state in umap will make the results reproducible across sessions but will disable parallelism and that will slow down training time"
      ],
      "metadata": {
        "id": "MlxiG1Q91Tbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cluster the Reduced Embeddings"
      ],
      "metadata": {
        "id": "49YMXwwM2eRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hdbscan import HDBSCAN\n",
        "\n",
        "# Fit the model and extract the clusters\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size = 50,\n",
        "    metric = \"euclidean\",\n",
        "    cluster_selection_method = \"eom\"\n",
        ").fit(reduced_embeddings)\n",
        "\n",
        "clusters = hdbscan_model.labels_\n",
        "\n",
        "# Check how many clusters were generated\n",
        "len(set(clusters))"
      ],
      "metadata": {
        "id": "J4m6E81C197U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the clusters\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# print first three documents in cluster 0\n",
        "cluster = 0\n",
        "for index in np.where(clusters==cluster)[0][:3]:\n",
        "  print(abstracts[int(index)][:300] + \"... \\n\")"
      ],
      "metadata": {
        "id": "bBax7hi33gc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the clusters\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# reduce 384-dimensional embeddings to two dimensions for easier visualization\n",
        "reduced_embeddings = UMAP(\n",
        "    n_components = 2,\n",
        "    min_dist = 0.0,\n",
        "    metric = \"cosine\",\n",
        "    random_state = 42\n",
        ").fit_transform(embeddings)\n",
        "\n",
        "# create dataframe\n",
        "df = pd.DataFrame(\n",
        "    reduced_embeddings,\n",
        "    columns=[\"x\", \"y\"]\n",
        ")\n",
        "df[\"title\"] = titles\n",
        "df[\"cluster\"] = [str(c) for c in clusters]\n",
        "\n",
        "# select outlier and non-outlier clusters\n",
        "to_plot = df.loc[df.cluster != \"-1\", :]\n",
        "outliers = df.loc[df.cluster == \"-1\", :]"
      ],
      "metadata": {
        "id": "OVJKLc6N33z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot outliers and non-outliers separately\n",
        "plt.scatter(\n",
        "    outliers.x,\n",
        "    outliers.y,\n",
        "    c = outliers.cluster.astype(int),\n",
        "    alpha = 0.6,\n",
        "    s = 2,\n",
        "    cmap = 'tab20b'\n",
        ")\n",
        "\n",
        "plt.scatter(\n",
        "    to_plot.x,\n",
        "    to_plot.y,\n",
        "    c = to_plot.cluster.astype(int),\n",
        "    alpha = 0.6,\n",
        "    s = 2,\n",
        "    cmap = 'tab20b'\n",
        ")\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5QUCHTGa477M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text clustering to topic modelling\n",
        "\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# train the model by using previously defined models\n",
        "topic_model = BERTopic(\n",
        "    embedding_model = embedding_model,\n",
        "    umap_model = umap_model,\n",
        "    hdbscan_model = hdbscan_model,\n",
        "    verbose = True\n",
        ").fit(abstracts, embeddings)"
      ],
      "metadata": {
        "id": "9fB-HdxA6Qwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "DkCuR0V88KIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic(0)"
      ],
      "metadata": {
        "id": "Tzjmeysz8pn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.find_topics(\"topic modelling\")"
      ],
      "metadata": {
        "id": "FKUMC6AE8tAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic(24)"
      ],
      "metadata": {
        "id": "zH0BGTkj8zRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.topics_[titles.index(\"BERTopic: Neural topic modeling with a class-based TF-IDF procedure\")]"
      ],
      "metadata": {
        "id": "H_VMZXMz83xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize topics and documents\n",
        "\n",
        "fig = topic_model.visualize_documents(\n",
        "    abstracts.to_list(),\n",
        "    topics = clusters,\n",
        "    reduced_embeddings = reduced_embeddings,\n",
        "    labels = titles.to_list(),\n",
        "    width = 1200,\n",
        "    hide_annotations = True\n",
        ")"
      ],
      "metadata": {
        "id": "g53-hWTL9F7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wJD8E5tc9hhk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}